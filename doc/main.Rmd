---
title: "Project 4 - Collaborative Filtering Algorithm"
author: "Team 6"
date: "4/18/2017"
output: pdf_document
---


## Step 0: Load the packages, and get the directories

```{r}
getwd()
```

## Step 1: Load and process the data

Here, we load in all the data we need for this project. There are four datasets, and they are respectively the testing and training sets of Microsoft Web data and Movie data.

```{r}
movie_train<-read.csv("../data/data_sample/eachmovie_sample/data_train.csv")
movie_test<-read.csv("../data/data_sample/eachmovie_sample/data_test.csv")

MS_train<-read.csv("../data/data_sample/MS_sample/data_train.csv")
MS_test<-read.csv("../data/data_sample/MS_sample/data_test.csv")
```


After loading the datasets, we transform the datasets so that they can be manipulated and applied in various functions later.

Here is how we transform the movie dataset.
```{r}
Transformer <- function(data.set){
  columns.data <- sort(unique(data.set$Movie))
  rows.data <- sort(unique(data.set$User))
  Table_ <- matrix(NA,nrow = length(rows.data), ncol = length(columns.data))
  for(i in 1:length(columns.data)){
    col.name <- columns.data[i]
    index <- which(data.set$Movie == col.name)
    scores <- data.set[index,4] #Scores
    users <- data.set[index,3] #Users
    index2 <- which(rows.data %in% users)
    Table_[index2,i] = scores
    Table_[!index2,i] = NA
  }
  colnames(Table_) <- columns.data
  rownames(Table_) <- rows.data
  return(Table_)
}

m_train<-Transformer(movie_train)
m_test<-Transformer(movie_test)

## add the missing columns to test data
miss_index<-numeric(length(train.index))
for(i in 1:length(train.index)){
  miss_index[i]<-train.index[i]%in%test.index
}
m_index<-which(miss_index == 0)
train.index[m_index]
m_matrix<-matrix(NA, ncol = 22, nrow = 5055)
colnames(m_matrix)<-m_index
m_test<-cbind(m_matrix, m_test)
m_test<-m_test[, order(as.numeric(colnames(m_test)))]


write.csv(m_train, "Movie_data_train.csv")
write.csv(m_test, "Movie_data_test.csv")
```

Here is how we transform the Microsoft dataset.

```{r}
MS.Data.Transform.Function <- function(data){
  usernum <- which(data$V1 == 'C')
  userid <- data$V2[usernum]
  vrootid <- unique(data$V2[which(data$V1 == 'V')])
  table <- matrix(0, nrow=length(userid), ncol=length(vrootid))
  rownames(table) <- as.character(userid)
  colnames(table) <- as.character(vrootid)
  for (i in 1:length(usernum)){
    start.indx <- usernum[i]
    if (i != length(usernum)){
      end.indx <- usernum[i+1]
    }
    else {
      end.indx <- nrow(data)+1
    }
    
    userid_mat <- as.character(userid[i])
    
    for (j in (start.indx+1):(end.indx-1)){
      vrootid_mat <- as.character(data$V2[j])
      table[userid_mat, vrootid_mat] <- 1
    }
  }
  return(table)
}

transformed.traindata <- MS.Data.Transform.Function(MS_train)
transformed.testdata <- MS.Data.Transform.Function(MS_test)
write.csv(transformed.traindata,file = "MS_data_train.csv")
write.csv(transformed.testdata,file = "MS_data_test.csv")
```


## Step 2: Create the required functions of Memory-based Algorithm and implemented them

First of all, We created three functions for similarity weighting.

1. Spearman Correlation:

```{r}
MS_data <- read.csv("../data/MS_data_train.csv")

new_movie_data <- read.csv("../data/Movie_data_train.csv")

weight_Spearman <- function(data1, dataset_name) {
  write_file_name <- paste0(dataset_name ,"_Spearman_weights" , ".csv")
  
  transformeddata <- t(data1)
  
  weights <-  cor(transformeddata, method = "spearman")
  
  write.csv(weights, write_file_name)
}

weight_Spearman(MS_data, "MS")

weight_Spearman(new_movie_data, "Movie")
```


2. Mean-square-difference:

```{r}
# Reduced the computational time significantly by assigning arguments outside 
# the loops, initiating the output matrix r with NA first, and using apply 
# to mean() outside the loops.

m_train<-as.matrix(m_train)

mean_sq_diff <- function(matrix){
  
  matrix[is.na(matrix)] = 0
  usermean = apply(matrix,1,mean)
  
  ncolrow = nrow(matrix)
  w <- matrix(rep(NA), ncolrow, ncolrow)
  rownames(w) = rownames(matrix)
  colnames(w) = rownames(matrix)
  
  for (r in 1:ncolrow){
    for (c in 1:ncolrow){
      if (r<c){
        w[r,c] <- (usermean[r]-usermean[c])^2
      }
      else if(r==c){
        w[r,c] <- 0
      }
      else {
        w[c,r]<-w[r,c]
      }
    }
  }
}
```


3. SimRank:

```{r}
MS.train.data <- read.csv("MS_data_train.csv",header=T)
MS.test.data <- read.csv("MS_data_test.csv",header=T)

# Substring the column name
substr.colname <- function(dataset){
  names <- colnames(dataset)
  for(i in 1:ncol(dataset)){
    names[i] <- substr(names[i], start = 2, stop = nchar(names[i]))
  }
  return(names)
}

# SimRank Function
generate.simrank.mat <- function(c = 0.8, k = 5, traindata = MS.train.data, 
                                 testdata = MS.test.data){
  MS.train.data = traindata
  MS.test.data = testdata
  
  # Pre process the train and test data
  rownames(MS.train.data) <- MS.train.data[,1]
  MS.train.data <- MS.train.data[,-1]  # variable name are exclude from the data area
  rownames(MS.test.data) <- MS.test.data[,1]
  MS.test.data <- MS.test.data[,-1]
  colnames(MS.train.data) <- substr.colname(MS.train.data)
  colnames(MS.test.data) <- substr.colname(MS.test.data)
  
  # Generate the bipartite graph
  bipartite.graph <- list()
  for(i in 1:nrow(MS.train.data)){
    bipartite.graph[[i]] <- colnames(MS.train.data)[which(MS.train.data[i,]==1)]
  }
  n <- length(bipartite.graph)
  t_MS.train.data <- as.data.frame(t(MS.train.data))
  for(i in 1:nrow(t_MS.train.data)){
    bipartite.graph[[n+i]] <- colnames(t_MS.train.data)[which(t_MS.train.data[i,]==1)]
  }
  tail(bipartite.graph)
  # Nodes name = user ID + vroots
  nodes.name <- as.numeric(c(rownames(MS.train.data),colnames(MS.train.data)))
  
  # Construction of Graph Matrix
  graph.matrix <- matrix(0,nrow = length(nodes.name),ncol = length(nodes.name))
  matrix1 <- matrix(0,nrow = nrow(MS.train.data),ncol = nrow(MS.train.data))
  matrix3 <- t(MS.train.data)
  matrix4 <- matrix(0,nrow = ncol(MS.train.data),ncol = ncol(MS.train.data))
  matrix5 <- cbind(matrix3,matrix4)
  graph.matrix <- cbind(matrix1,MS.train.data)
  colnames(graph.matrix) <- nodes.name
  colnames(matrix5) <- nodes.name
  graph.matrix <- rbind(graph.matrix,matrix5)
  dim(graph.matrix)
  
  # Column-normalized matrix
  colnum <- colSums(graph.matrix)
  for (i in 1:ncol(graph.matrix)){
    graph.matrix[,i] <- graph.matrix[,i]/colnum[i]
  }
  
  # Iteration Mtd of SimRank
  W <- as.matrix(graph.matrix)
  simrank.matrix.pre <- matrix(0, nrow = length(nodes.name), ncol = length(nodes.name))
  diag(simrank.matrix.pre) <- 1 
  simrank.matrix <- simrank.matrix.pre

  for(i in 1:k){
    simrank.matrix.pre <- simrank.matrix
    simrank.matrix <- c * (t(W) %*% simrank.matrix.pre %*% W)
    diag(simrank.matrix) <- 1
    print(i)
  }
  dim(simrank.matrix)
  
  # Extraction of User Part
  simrank.fin.matrix <- simrank.matrix[1:nrow(MS.train.data),1:nrow(MS.train.data)]
  dim(simrank.fin.matrix)
  
  return(simrank.fin.matrix)
}


simrank.fin.matrix <- generate.simrank.mat(c = 0.8, k = 5, traindata = MS.train.data, 
                                       testdata = MS.test.data)
write.csv(simrank.fin.matrix, file = "simrank.fin.matrix.csv")
```


Secondly, after having the similarity weights, we write out the functions which are able to find the neighbors and make prediction for those weights. Then, we find the neighbors and make the predictions for them.

Notes: Since the datasets for the similarity weights we calculated are too large to upload onto the github, we just load all the datasets from our local directory. Those datasets, however, have been already been uploaded onto the google drive we created. You can download the data from here if you want:

https://drive.google.com/drive/u/1/folders/1AEJbWGCD507uqtJ5hWNZD9nv8hRBv44v

Here are the find neighbors function and the prediction function:

```{r}
find_neighbours<-function(sim_mat,method='combined',threshold=NA,n=NA){
  
  #####Input: similarity matrix
  #####       method: weighthres, bestn, combined
  #####       threshold
  #####Output: index of neighbours
  
  stopifnot(method=='weighthres'|method=='combined'|method=='bestn')
  
  neighbours<-list()
  
  if (method=='weighthres'){
    for (i in 1:nrow(sim_mat)){
      sel<-ifelse(abs(sim_mat[i,])>threshold,1,0)
      ind<-which(sel==1)
      neighbours[[i]]<-ind[which(ind!=i)]
    }
  }else{
    if(method=='bestn'){
      for (i in 1:nrow(sim_mat)){
        ord<-order(abs(sim_mat[i,]),decreasing = T)
        neighbours[[i]]<-ord[which(ord!=i)][1:n]
      }
    }else{
      for(i in 1:nrow(sim_mat)){
        ind.w<-which(ifelse(abs(sim_mat[i,])>threshold,1,0)==1)
        sel.w<-ind.w[which(ind.w!=i)]
        ord.b<-order(abs(sim_mat[i,]),decreasing = T) 
        sel.b<-ord.b[which(ord.b!=i)][1:n]
        neighbours[[i]]<-intersect(sel.w,sel.b)
      }
    }
  }
  
  null <- (1:length(neighbours))[lapply(neighbours,length) == 0]
  if(length(null)!=0){
    for (i in 1:length(null)){
      index <- null[i]
      neighbours[[index]] <- null[i]
    }
  }
  
  return(neighbours)
}


prediction<-function(df,sim_mat,nbor_list){
  
  df.new<-df   #copy for calculating the mean by control NA.
  for (i in 1:nrow(df.new)){
    df.new[i,]<-ifelse(df.new[i,]==0,NA,df.new[i,])
  }
  df.mean<-matrix(rep(rowMeans(df.new,na.rm=T),each=ncol(df)),nrow=nrow(df),byrow = T)
  
  pred<-matrix(NA,ncol=ncol(df),nrow=nrow(df))
  
  for (i in 1:nrow(df)){
    ind<-nbor_list[[i]]
    nbors_mat<-df[ind,]
    w<-as.matrix(sim_mat[i,ind]/sum(sim_mat[i,ind]))
    nei.demean <- as.matrix(nbors_mat-df.mean[ind,])
    pred.a<- df.mean[i,1] + (w %*% (nei.demean))
    rm.ind<-which(df[i,]==1)
    pred.a[rm.ind]<-0
    pred[i,]<-pred.a
  }
  
  for (i in 1:nrow(df)){
    pred[i,]<-ifelse(pred[i,]<0,0,pred[i,])
  }  
  
  rownames(pred)<-rownames(df)
  colnames(pred)<-colnames(df)
  
  return(pred)
}
```


Then, we implement the function into all the weights we got.

```{r}
############## Movie data set
######## Spearman Correlation
Movie_weights <- read.csv("C:/StudyLife/Columbia/STAT 5243/Local Project 4/Each Movie Case/Movie_Spearman_weights.csv")

data1 <- read.csv("C:/StudyLife/Columbia/STAT 5243/Local Project 4/Each Movie Case/Movie_data_train.csv")

movietest <- read.csv("C:/StudyLife/Columbia/STAT 5243/Local Project 4/Each Movie Case/Movie_data_test.csv")
movietest <- movietest[,-1]

numeric_movietest <- NULL
for(i in 1:ncol(movietest)) {
  numeric_movietest <- cbind(numeric_movietest, as.numeric(movietest[,i]))
}

# neighbors of method bestn
Movie_neighbors <- find_neighbours(Movie_weights[,-1], method = "bestn" , n = 60)

movie_prediction <- prediction(data1[, -1], Movie_weights[, -1], Movie_neighbors)

# neighbors of method weighthres
movie_neighbors_weighthres <- find_neighbours(Movie_weights[,-1], method = "weighthres" , threshold = 0.3)
  
movie_prediction_weighthres <- prediction(data1[, -1], Movie_weights[, -1], movie_neighbors_weighthres)

# neighbors of method combined
movie_neighbors_combined <- find_neighbours(Movie_weights[,-1], method = "combined" , threshold = 0.3, n = 60)

movie_prediction_combined <- prediction(data1[, -1], Movie_weights[, -1], movie_neighbors_combined)


######### MSD
Movie_MSD_weights <- read.csv("C:/StudyLife/Columbia/STAT 5243/Local Project 4/Each Movie Case/Movie_msd_weight.csv")

# neighbors of method bestn
Movie_MSD_neighbors <- find_neighbours(Movie_MSD_weights[,-1], method = "bestn" , n = 60)

movie_prediction_MSD <- prediction(data1[, -1], Movie_MSD_weights[, -1], Movie_MSD_neighbors)


# neighbors of method weighthres
movie_MSD_neighbors_weighthres <- find_neighbours(Movie_MSD_weights[,-1], method = "weighthres" , threshold = 0.3)

movie_prediction_MSD_weighthres <- prediction(data1[, -1], Movie_MSD_weights[, -1], movie_MSD_neighbors_weighthres)

# neighbors of method combined
movie_MSD_neighbors_combined <- find_neighbours(Movie_MSD_weights[,-1], method = "combined" , threshold = 0.3, n = 60)

movie_prediction_MSD_combined <- prediction(data1[, -1], Movie_MSD_weights[, -1], movie_MSD_neighbors_combined)
```


## Step 3: Clustering



## Step 4: Evaluation

We firstly write out the functions for the two simulation methods, which are respectively Rank Score and MAE.

```{r}
## MAE
evaluation_mae <- function(pred_mat, Movie_test){
  ## function to calculate mean absolute error of predicted value
  ## Input: pred_mat - predicted value
  ##        Movie_test - test data matrix
  ## Output: MAE
  for (i in 1:nrow(pred_mat)){
    pred_mat[i,]<-ifelse(pred_mat[i,]==0,NA,pred_mat[i,])
  }
  mae <- mean(abs(pred_mat - Movie_test), na.rm = T)
  return(mae)
}

## Rank Score
rank_score<-function(pred,test,d,alpha){
  R_a<-rep(NA,nrow(test))
  R_a_max<-rep(NA,nrow(test))
  
  for(i in 1:nrow(test)){
    j<-order(pred[i,],decreasing = T)
    pred.s<-sort(pred[i,])
    numer<-ifelse(pred.s-d>0,pred.s-d,0) 
    R_a[i] <- sum( numer / 2^((j-1)/(alpha-1)))
    pred.s<-ifelse(test[i,]==1,1,pred.s)
    j<-order(pred.s,decreasing = T)
    pred.s<-sort(pred.s)
    denom<-ifelse(pred.s-d>0,pred.s-d,0) 
    R_a_max[i] <- sum( denom / 2^((j-1)/(alpha-1)))
  }
  R <- 100 * (sum(R_a) / sum(R_a_max))
  return(R)
}
```







